---
title: "Consumer Engagement Internet News"
author: "Thiha Naung"
date: "8/30/2021"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

This project is based on the data set of Consumer Engagement Internet News from DataCamp's Workspace Publication Competition. This dataset consists of data about news articles collected from September 3, 2019 until November 4, 2019. 

This dataset contains news articles from 13 news sources such as BBC, CNN, New York Times with share, reaction and comment counts each. From this dataset, I found 5 interesting questions.  

1. Which news source got the most people's interest (engagement) rate?  

2. Is it true that top articles get the most people’s interest rate and are there any news articles that are not rated as top articles but can get people’s interest rate?  

3. Which titles of articles did most people engage in?  

4. Type of words such as positive words, negative words, emotion words, are equally contained in top and not top articles or in interested or not interested articles?  

5. Which kind of positive and negative words most appeared in news articles during those days?  

Let's start exploring.
---
First import dataset and required libraries.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(GGally)
library(tidytext)
library(wordcloud2)
library(fmsb)
```


```{r , warning=FALSE, message=FALSE}
data <- read_csv("~/Desktop/R/data/data/news_articles.csv.gz")
```

Let's check the contents of the data.  

```{r}
glimpse(data)
```

The data format are correct.  

To know which news got most people’s interest rate, first, to seek out is whether these news sources share the same amount of articles. 

```{r}
data %>%
    group_by(source_name) %>%
    count(source_name) %>%
    arrange(desc(n))
```

What is "460.0"?  

```{r}
data %>% 
    filter(source_name == "460.0")
```

It is nothing and remove it.  
```{r}
data <- data %>%
    filter(!source_name == "460.0")
```

Are there NA values? Sure, it must contain NA values :D . How many NA values?  

```{r}
sapply(data, function(x){sum(is.na(x))})
```

Let's check whether counts of NA values from reaction,comment and share columns are same.  
```{r}
data %>%
    filter(is.na(engagement_reaction_count) & is.na(engagement_comment_count) & is.na(engagement_comment_plugin_count) & is.na(engagement_share_count)) %>%
    nrow()
```
Yes, they share the same rows. So drop NA values and combine engagement_comment_count and engagement_comment_plugin_count as they share same category in comment.  

```{r}
clean_at_counts <- data %>%
    filter(!(is.na(engagement_reaction_count) & is.na(engagement_comment_count) & is.na(engagement_comment_plugin_count) & is.na(engagement_share_count))) %>%
    mutate(comment_count = engagement_comment_count + engagement_comment_plugin_count) 
```

## Most people’s interest News Sources

```{r}
clean_at_counts %>% 
    # select required columns
    select("source_name", "engagement_reaction_count", "comment_count", "engagement_share_count") %>%
    group_by(source_name) %>%
    summarize(
        # Calculate the average counts for each sources
        average_reaction_count = mean(engagement_reaction_count),
        average_comment_count = mean(comment_count),
        average_share_count = mean(engagement_share_count),
        # sum for showing from largest to smallest at graph/ nothing more
        sum = average_reaction_count + average_comment_count + average_share_count,
    ) %>%
    ungroup() %>%
    # make source name as factor according to sum (from largest to smallest)
    mutate(source_name = fct_reorder(source_name, sum)) %>%
    gather(category, count, -c(source_name,sum)) %>%
    ggplot(aes(x = source_name, y = count, fill = category)) +
    geom_col() +
    coord_flip() +
    theme_bw() +
    labs(title = "Average Engagements of News Sources", y = "Average Counts per news", x = "", fill = "Category") +
    scale_fill_manual(labels = c("Average Comment Counts","Average Reaction Counts","Average Share Counts"), values = c("orchid","green","royalblue")) +
    theme(legend.position = c(0.8,0.2))
```

Summation will not give the correct amount as the news sources do not share the same amount of news articles and so, I calculated with mean values with respect to each news sources. 

So, according to the graph, CNN and New York Times have the most people's interest rate than others during this period and interesting thing is ESPN has no engagements. Maybe people watch sports news from live channels and read less from websites. Next is Reuters has too many share counts compared to comment and reaction count.  

--- 

## Top Articles ??

"top_article" is said "value indicating if the article was listed as a top article on publisher website" but one interesting thing about top article is, though there was no comment, reaction, share,but said to top articles.  


```{r}
data %>%
    filter(top_article == 1 & engagement_comment_count == 0 & engagement_reaction_count == 0 & engagement_share_count == 0 & engagement_comment_plugin_count == 0) %>%
    group_by(source_name) %>%
    summarize(number_of_top_articles_without_engagements = n())
```

Yes. there are too many top articles that have no people's engagements. 

But, on average, do top articles get the most engagements?  

```{r}
clean_at_counts$top_article <- factor(clean_at_counts$top_article, levels = c(1,0), labels = c("Yes","No"))  
clean_at_counts[,c(11,12,14,16)] %>%
    gather(category, count, -top_article) %>%
    ggplot(aes(x = category, count, fill = top_article)) +
    geom_boxplot() +
    scale_y_log10() +
    theme_bw() +
    labs(x = "", y = "Log of counts", fill = "Top Article") +
    scale_fill_manual(values = c("royalblue", "green"))
```


I made the y-axis log scale, as the range of outliers is too much. According to this boxplot, on average, it is true that top articles have greater number of people interest rate but there are outliers in not top articles that beats the maximum numbers of top articles. To be sure, the top articles have a greater number of people's interest rate, let's check.  


```{r}
t.test(comment_count ~ top_article, data = clean_at_counts, conf.level = 0.99)
t.test(engagement_reaction_count ~ top_article, data = clean_at_counts, conf.level = 0.99)
t.test(engagement_share_count ~ top_article, data = clean_at_counts, conf.level = 0.99)
```


Yes, it is true with even confidence level 99%. 


But there are outliers in not rating as top article, and so let's check if there are articles rating as not top article that beats the maximum engagement numbers of top article according to news sources.  


```{r}
clean_at_counts %>%
    select("source_name", "top_article", "engagement_reaction_count", "comment_count", "engagement_share_count") %>%
    group_by(source_name, top_article) %>%
    summarize(
        Comment = max(comment_count),
        Reaction = max(engagement_reaction_count),
        Share = max(engagement_share_count)
    ) %>%
    gather(category, max_count, -c(source_name, top_article)) %>%
    spread(top_article, max_count) %>%
    mutate(difference = `Yes` - `No`) %>%
    filter(source_name != "ESPN") %>%
ggplot(aes(source_name, difference)) +
    geom_col(fill = "royalblue") +
    coord_flip() +
    facet_wrap(~ category,nrow=2, scales = "free_x") +
    labs(x = "News Sources", y = "", title = "Difference between maximum engagement numbers of \ntop article and not rating as top article") +
    theme_bw()
```

Interestingly, one CNN news article that has not been rated as top article has many more engagement counts than those that are rated as top articles from CNN. 
What is it?  What is it about?  


```{r}
data.frame(clean_at_counts %>%
    filter(source_name == "CNN" & engagement_reaction_count == max(engagement_reaction_count))) %>%
    select(description, url)
```

It is about the former US president, Jimmy Carter who turns the age of 95 and it is also the maximum reaction and share counts among all the news. 

---

Next is not from questions.  
There are 3 separated parameters; comment, share, and reaction. Are they associated? In general sense, it may be positively correlated. Let's look at the data set. 

```{r , warning=FALSE, message=FALSE}
colnames(clean_at_counts)
ggpairs(clean_at_counts[,c(11,12,14,16)], columns = 2:4, aes(color = top_article, alpha = 0.5)) +
    scale_color_manual(values = c("blue","green")) +
    scale_x_log10() + 
    scale_y_log10() 
```

Blue dots are top articles and green are not. Comment and reaction counts are positively strong correlated. It makes sense. So, I will combine these three columns.  

---

## Titles of articles that most people engaged 

I think to determine whether an article is engaged/interesting or not, top articles cannot be used because as shown above there are articles that have not been rated as top articles but had most people engaged counts. And the reaction, share and comment counts are positively correlated and I will combine these columns and if summation is zero, this article has no engagement. 

Ok, let's sum up.  

```{r}
total_engagement_df <- clean_at_counts %>%
    mutate(total_engagements = engagement_reaction_count + comment_count + engagement_share_count) %>%
    select("news_id" = "...1",2:11,17) 

summary(total_engagement_df$total_engagements)
```

Median number is only 13 !  

```{r}
total_engagement_df <- total_engagement_df %>%
    mutate(interested = ifelse(total_engagements > 0, "Yes","No"),
           interested = as.factor(interested))

table(total_engagement_df$interested)
```

There are 2485 articles that have no engagement. Let's check how many NA values. 

```{r}
sapply(total_engagement_df, function(x){sum(is.na(x))})
```

There are only 2 NA values in title column.  

> There is a problem with wordcloud2 in rendering to html output that allows only one picture and does not allow second picture, so I skip my first picture of showing wordcloud with one word.  

```{r}
two_words <- total_engagement_df %>%
    filter(!is.na(title) & interested == "Yes") %>%
    select(news_id, title) %>%
    unnest_tokens(ngram, title, token="ngrams", n=2, stopwords = stop_words$word) %>% filter(!ngram %in% c("york times","street journal","wall street")) %>%
    count(ngram) %>%
    arrange(desc(n)) %>%
    top_n(50)
    
wordcloud2(two_words)
```

> first run, the words "york times","street journal" contain a lot, so I removed them. 

Yes, these are the most frequent topic from all engaged news articles.  

---

## Emotional words !!  

Emotional words equally contain in top article news or in not a top article news.  

Let's filter the columns.  
At first, I did that way.--->
> So, most words contain in content column but there are many NAs in that columns and I replaced them from description column.  

But it didn't work in calculating at polarity scores from description as it contains few words and make wrong results.  

```{r}
description_text <- total_engagement_df %>%
    select(news_id, source_name, top_article, interested, text = "content") %>%   
    filter(!is.na(text))
```

Joined from "NRC" words that are classified as 10 categories;positive, negative, fear, trust, etc.

```{r}
nrc_words <- description_text %>%
    select(-2) %>% 
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    inner_join(get_sentiments("nrc"))

head(nrc_words)
```

I want to classify them with top_article category and engagement/interesting category.  

```{r}
top_article_nrc <- nrc_words %>%
    group_by(news_id, top_article) %>%
    count(sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    gather(category, score, -c("news_id","top_article")) %>%
    group_by(top_article, category) %>%
    summarize(average_score = mean(score)) %>%
    spread(top_article, average_score)
head(top_article_nrc)

interested_nrc <- nrc_words %>%
    group_by(news_id, interested) %>%
    count(sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    gather(category, score, -c("news_id","interested")) %>%
    group_by(interested, category) %>%
    summarize(average_score = mean(score)) %>%
    spread(interested, average_score)
head(interested_nrc)
```

At first, I tried with chartJSradar but it does not appear when rendering to html. It is easy to use and just one line of code. So, try to make chartJSradar to png but cannot find the solution. So, I make the radar chart from library "fmsb". It is too handy and graph is not that pretty. Are there any methods?  


Let's see on graphs.  

```{r, out.width="100%"}
radar_df <- function(df) {
    trans <- data.frame(t(df[,2:3]))
    colnames(trans) <- df$category
    min_max <- data.frame(matrix(rep(c(2.5,0),10), nrow=2))
    colnames(min_max) <- df$category
    radar <- rbind(min_max, trans)
    return(radar)
}

top_article <- radar_df(top_article_nrc)
top_interest <- radar_df(interested_nrc)

op <- par(mar = c(1, 2, 2, 2))
par(mfcol = c(1,2))
radarchart(top_article, caxislabels = c(seq(0,2.5,0.5)), seg=5, plty=1, pcol=c("red","blue"), title="Word Contents of top and \nnot top articles",axistype = 3)

legend("bottom",legend = rownames(top_article[-c(1,2),]), col = c("red","blue"), pch = 20,bty = "n",text.col = "black", cex = 1, pt.cex = 1.5,horiz=TRUE,)

radarchart(axistype=3, top_interest, caxislabels = c(seq(0,2.5,0.5)), seg=5, plty=1, pcol=c("red","blue"), title="Word Contents of Engaged and \nNot Engaged articles")

legend("bottom",legend = rownames(top_interest[-c(1,2),]), col = c("red","blue"), pch = 20,bty = "n",text.col = "black", cex = 1, pt.cex = 1.5,horiz=TRUE)

par(op)
```

It seems all the contents have same average amount of all kinds of words. Yes, that makes sense as articles contain all types of feelings. It is nice to see positive words take most.  

---

## Most appeared Positive and Negative Words From All Articles  

Which positive and negative words can be seen from these articles during these days?  

Let's find.  

```{r}
description_text %>%
    select(text) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    inner_join(get_sentiments("bing")) %>%
    group_by(sentiment) %>%
    count(word) %>%
    group_by(sentiment) %>%
    arrange(desc(n)) %>%
    top_n(20) %>%
    mutate(word = fct_reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
        geom_col() +
        coord_flip() +
        labs(y = "Count of Words", x = "Words", title = "Words that are most appeared in News Articles", fill = "") + 
        theme_bw() +
        scale_fill_manual(labels = c("Negative word","Positive word"), values = c("red","royalblue")) +
        theme(legend.position = c(0.8,0.2))

```


Yes, these are the words that occur frequently in those news articles. But one word has only rough analysis. Positive words may mix with negative words. Let's find polarity in each articles.    

```{r}
pos_neg_words <- description_text %>%
  select(news_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(news_id) %>%
  count(sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(diff = positive - negative) %>%
  inner_join(description_text, by="news_id")


pos_neg_words %>%
  group_by(source_name) %>%
  ggplot(aes(source_name, y = diff, fill=source_name)) +
  geom_boxplot() + 
  geom_hline(yintercept=0, linetype = "dotted", color = "darkblue") +
  labs(x = "", y = "Polarity of News", title = "Polarity of News per News Sources") +
  coord_flip() +
  theme_bw() +
  theme(legend.position="none")
```

Most of the news from new sources have negative polarity on average. 
Let's test the differences in polarity are true or not.  
First find the maximum and minimum numbers of polarity.  

```{r}
max(pos_neg_words$diff)
min(pos_neg_words$diff)
```
Range is 8 to -8. 
Let see the news that contains most positive words from content.  

```{r}
data.frame(pos_neg_words %>%
    filter(diff == 8) %>%
    inner_join(total_engagement_df[,c("news_id","url")], by="news_id") %>%
    select(source_name, url))
```
Yes, they are not bad news.  

Next see the news that contains most negative words from content.  

```{r}
data.frame(pos_neg_words %>%
    filter(diff == -8) %>%
    inner_join(total_engagement_df[,c("news_id","url")], by="news_id") %>%
    select(source_name, url))
```
Imm...one is true, it is about saudi_arabia drone attack. Next is why. 

```{r}
pos_neg_words %>%
    filter(diff == -8 & news_id == 1890) %>%
    select(text) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    inner_join(get_sentiments("bing"))
```

It contains "afraid","difficult", etc and no positive words.  

---

That's all. It is just general analysis as I am just starting my data science journey. Thanks a lot DataCamp courses. Because of Datacamp, I can analyse as above within just three months, not from computer science or maths background. In my analysis, there is no regression models, no web scraping, no image analysis, no time series analysis that can be done above data set. I need to learn a lot. Thank you so much for your precious time for reading.  Thanks a lot.  

---













